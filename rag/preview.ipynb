{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1ec0f76-970a-47cd-aa1e-0a2e1a6ad92f",
   "metadata": {},
   "source": [
    "# Preview from LangChain\n",
    "\n",
    "## Step by step code\n",
    "\n",
    "1. get LangSmith API Key from environment\n",
    "2. set up anthropic key for chat model\n",
    "3. set up embedding model for embeddings\n",
    "4. select vector database\n",
    "5. set up document loader and split up\n",
    "\n",
    "## Set up API keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8825c3-e6e4-4541-aa62-fc862ba42bc6",
   "metadata": {},
   "source": [
    "## Tutorials\n",
    "LangChain RAG tutorial document\n",
    "Part 1\n",
    "https://python.langchain.com/docs/tutorials/rag\n",
    "\n",
    "Part 2\n",
    "extends the implementation to accommodate conversation-style interactions and\n",
    "multi-step retrieval processes.\n",
    "https://python.langchain.com/docs/tutorials/qa_chat_history/\n",
    "\n",
    "LangChain document loader for GitHub Repo\n",
    "https://python.langchain.com/docs/integrations/document_loaders/github/\n",
    "\n",
    "LangChain document loader for Git Repository\n",
    "https://python.langchain.com/docs/integrations/document_loaders/git/\n",
    "\n",
    "LangChain document loader for Source Code (e.g. Python)\n",
    "https://python.langchain.com/docs/integrations/document_loaders/source_code/\n",
    "\n",
    "LangSmith evaluation for a chatbot\n",
    "https://docs.smith.langchain.com/evaluation/tutorials/evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00262b7-72d9-4472-bdcb-f1a85e3321eb",
   "metadata": {},
   "source": [
    "* [X] set up LangSmith key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f79f4383-61f6-41d8-9536-0873e426f630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter API key for LangSmith to enable tracing:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter API key for LangSmith to enable tracing: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36908430-f05f-4d6f-ae67-cafb09ed39db",
   "metadata": {},
   "source": [
    "* [X] set up Anthropic key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac315b7b-3b72-4f20-a641-9b2b498a7100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter API key for Anthropic:  ········\n"
     ]
    }
   ],
   "source": [
    "if not os.environ.get(\"ANTHROPIC_API_KEY\"):\n",
    "  os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter API key for Anthropic: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"claude-3-5-haiku-latest\", model_provider=\"anthropic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7032cc2a-ae18-40bf-86aa-08270b25fc32",
   "metadata": {},
   "source": [
    "* [X] set up Google gemini as embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "278e39ad-8e1c-44eb-9502-0e7a84d36964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter API key for Google Gemini:  ········\n"
     ]
    }
   ],
   "source": [
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2fbf689-36a9-404c-8f32-c367e7675587",
   "metadata": {},
   "source": [
    "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
    "\n",
    "if not os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\"):\n",
    "  os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter API key for HuggingFace Hub: \")\n",
    "\n",
    "model = \"microsoft/codebert-base\"\n",
    "embeddings = HuggingFaceEndpointEmbeddings(\n",
    "    model=model,\n",
    "    task=\"feature-extraction\"\n",
    "    # huggingfacehub_api_token=\"my-api-key\",\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11fc1054-ede8-4f61-834c-9af068e26f17",
   "metadata": {},
   "source": [
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "\n",
    "inference_api_key = getpass.getpass(\"Enter API key for HuggingFace: \")\n",
    "model = \"microsoft/graphcodebert-base\" \n",
    "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=inference_api_key, model_name=model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad7a684-56c6-40e1-8a64-b86089a6e12c",
   "metadata": {},
   "source": [
    "* [X] set up vector database \n",
    "\n",
    "# Choosing database\n",
    "\n",
    "Load, chunk, split, embed and vectorize code data and document data into database\n",
    "\n",
    "## Candidates\n",
    "\n",
    "1. cassandra\n",
    "2. open search https://opensearch.org/platform/os-search/vector-database/\n",
    "3. Pinecone\n",
    "4. MongoDB\n",
    "5. PostgreSQL\n",
    "6. [X] Chroma, locally hosted with sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5954c2e3-3c86-41f1-9ce2-d76e5fb88ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "#huggingface_store = Chroma(\n",
    "#    collection_name=\"hf_python_tech_credit\",\n",
    "#    embedding_function=embeddings,\n",
    "#    persist_directory=\"./chroma_langchain_db\",\n",
    "#)\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"python_tech_credit\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")\n",
    "\n",
    "document_store = Chroma(\n",
    "    collection_name=\"document_tech_credit\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebf0591-3020-489b-9563-00a12e3a20bb",
   "metadata": {},
   "source": [
    "* [ ] Import and load a GitHub Repo as a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8ee1cb4-7571-4e12-a190-58e53b4d6be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter ACCESS_TOKEN for GitHub:  ········\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import GithubFileLoader\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "from code_splitter import Language, TiktokenSplitter\n",
    "\n",
    "if not os.environ.get(\"GITHUB_PERSONAL_ACCESS_TOKEN\"):\n",
    "   os.environ[\"GITHUB_PERSONAL_ACCESS_TOKEN\"] = getpass.getpass(\"Enter ACCESS_TOKEN for GitHub: \")\n",
    "\n",
    "# Load and chunk contents of the github repo\n",
    "loader = GithubFileLoader(\n",
    "    repo=\"ameliarogerscodes/TC-Examples\",  # the repo name\n",
    "    branch=\"main\",  # the branch name\n",
    "    # access_token=ACCESS_TOKEN, # delete/comment out this argument if you've set the access token as an env var.\n",
    "    github_api_url=\"https://api.github.com\",\n",
    "    # parser=LanguageParser(language=Language.PYTHON, parser_threshold=200),\n",
    "    file_filter=lambda file_path: file_path.endswith(\n",
    "        \".py\"\n",
    "    ),  # load all python files.\n",
    ")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f4df20-f761-4288-b10a-574e3a2a21f4",
   "metadata": {},
   "source": [
    "* [ ] test documents content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ad2951-9597-42c1-b399-1f7caa8e9949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': 'MVC/model.py', 'sha': 'c8d55a49e54ee3256ceb480618d10ccdd4b58b19', 'source': 'https://api.github.com/ameliarogerscodes/TC-Examples/blob/main/MVC/model.py'}\n"
     ]
    }
   ],
   "source": [
    "print(documents[7].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eb1b2a-4c2a-49f7-beae-e5200e0eee85",
   "metadata": {},
   "source": [
    "* [ ] map metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0d2c29e-4580-4232-b678-8db2d4028644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Step 1: Load the JSON metadata from a file (adjust path accordingly)\n",
    "with open('./repo_metadata.json', 'r', encoding='utf-8') as f:\n",
    "    json_metadata_list = json.load(f)\n",
    "\n",
    "# The sample JSON will be like a list of dicts, for example:\n",
    "# [\n",
    "#   {\"path\": \"src/pybreaker.py\", \"type\": \"source\", \"tech-credit\": \"Circuit Breaker\", \"tech_credit_description\": \"good design\"},\n",
    "#   {\"path\": \"test/unitest_pybreaker.py\", \"type\": \"test\", \"tech-credit\": \"Circuit Breaker\", \"tech_credit_description\": \"good design\"}\n",
    "# ]\n",
    "\n",
    "# Step 2: Create a dictionary mapping from path to metadata (excluding 'path' key)\n",
    "metadata_map = {\n",
    "    entry['path']: {k: v for k, v in entry.items() if k != 'path'}\n",
    "    for entry in json_metadata_list\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95abeffb-07c5-4036-95ef-320590c19b61",
   "metadata": {},
   "source": [
    "* [X] use code splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a26421b-1478-4354-a169-62389fdd8d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use code-splitter \n",
    "# https://pypi.org/project/code-splitter/\n",
    "python_splitter = TiktokenSplitter(Language.Python, max_size=200)\n",
    "\n",
    "all_splits = [\n",
    "    Document(\n",
    "        page_content=(\n",
    "            # \"# ===== code structure =====\\n\"\n",
    "            # + \"\\n\".join(f\"# {line}\" for line in splits.subtree.splitlines())\n",
    "            # + \"\\n\\n\"\n",
    "            splits.text\n",
    "        ),\n",
    "        metadata=metadata_map.get(doc.metadata.get('path'), {})\n",
    "    )\n",
    "    for doc in documents \n",
    "    for splits in python_splitter.split(doc.page_content.encode(\"utf-8\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280decb5-9f05-4303-9964-b10df988ab5c",
   "metadata": {},
   "source": [
    "* [ ] print page_content and metadata for a split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f9958ae-a050-4d27-b065-c6d9c63ec640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@property\n",
      "    @abstractmethod\n",
      "    def counter(self) -> int:\n",
      "        \"\"\"Override this method to retrieve the current value of the failure counter.\"\"\"\n",
      "\n",
      "    @property\n",
      "    @abstractmethod\n",
      "    def opened_at(self) -> datetime | None:\n",
      "        \"\"\"Override this method to retrieve the most recent value of when the circuit was opened.\"\"\"\n",
      "\n",
      "    @opened_at.setter\n",
      "    def opened_at(self, datetime: datetime) -> None:\n",
      "        \"\"\"Override this method to set the most recent value of when the circuit was opened.\"\"\"\n",
      "\n",
      "\n",
      "class CircuitMemoryStorage(CircuitBreakerStorage):\n",
      "{'type': 'source', 'tech_credit': 'Circuit Breaker', 'tech_credit_description': 'Enhance system resilience by dynamically detecting service failures and preventing cascading issues, especially in distributed systems.'}\n"
     ]
    }
   ],
   "source": [
    "print(all_splits[34].page_content)\n",
    "print(all_splits[34].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996ffe88-73d0-4bc0-9b33-299d1139db49",
   "metadata": {},
   "source": [
    "* [ ] load, split and embed documents into vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bfc96a-85ce-4a0b-9da0-4b7d00c46e23",
   "metadata": {},
   "source": [
    "* [x] index chunks for code vector db, DO NOT LOAD TWICE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57fa4981-55e9-40d1-9df3-b1891d549073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index chunks\n",
    "code_embed_index = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f39c77-6a6d-4979-a565-3bdc65236170",
   "metadata": {},
   "source": [
    "* [ ] load, chunk, split and embed documents about technical credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b926570a-74c5-4b24-8b75-a830ab290d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "bs4_strainer = bs4.SoupStrainer(\n",
    "    class_=(\"article-header__section\", \"article-header__topic-and-issue-section\",\n",
    "            \"article-header article-header__title\", \"article-header__subtitle\",\n",
    "        \t\"article-header__meta\",\n",
    "        \t\"article-table-of-contents\",\n",
    "        \t\"article-contents\",\n",
    "        \t\"article-footer\")\n",
    ")\n",
    "\n",
    "doc_loader = WebBaseLoader(\n",
    "    web_paths=(\"https://cacm.acm.org/opinion/technical-credit/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4_strainer\n",
    "    ),\n",
    ")\n",
    "\n",
    "text_documents = doc_loader.load()\n",
    "# recurisive splitter , 7 , all splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57093c26-c49c-454b-964d-985d99c027e1",
   "metadata": {},
   "source": [
    "* [X] test the web page document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "775b323a-607b-478c-aeec-ca538dd21511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 14484\n",
      "Opinion\n",
      "\n",
      "Computing Profession \n",
      "\n",
      "\n",
      "Balancing initial investment and long-term results in the software development process.\n",
      "\n",
      "\n",
      "\t\t\t\tBy Ian Gorton, Alessio Bucaioni, and Patrizio Pelliccione \n",
      "\n",
      "Posted Dec 26 2024 \n",
      "\n",
      "\n",
      "\n",
      "What Is Technical Credit?\n",
      "Technical Credit in Practice\n",
      "A Research Agenda for Technical Credit\n",
      "Conclusion\n",
      "References\n",
      "Footnotes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Technical debt (TD) is an established concept in software engineering encompassing an unavoidable side effect of software development.3 It arises due to tight s\n"
     ]
    }
   ],
   "source": [
    "assert len(text_documents) == 1\n",
    "print(f\"Total characters: {len(text_documents[0].page_content)}\")\n",
    "print(text_documents[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2dcd6c-3469-4211-95bd-eadd65f86743",
   "metadata": {},
   "source": [
    "* [ ] split the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1963b0b9-1ac0-44d4-8c7e-8246eb5e3979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split article into 20 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "text_splits = text_splitter.split_documents(text_documents)\n",
    "\n",
    "print(f\"Split article into {len(text_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8341841c-23d3-40c8-83d0-b2187ac2c44a",
   "metadata": {},
   "source": [
    "* [X] embed the documents into vector database, DO NOT LOAD TWICE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afd11df3-bfa5-418d-bd78-60ad0497b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids = document_store.add_documents(documents=text_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6178740-773d-432d-a533-45e2a4a21b4e",
   "metadata": {},
   "source": [
    "# RAG System Part\n",
    "## Customize Prompt\n",
    "## Define nodes and graphs in the rag system\n",
    "1. [X] retrieve code and metadata\n",
    "2. [X] retrieve academic documents\n",
    "3. [X] send message to LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93143ad-64df-4240-882f-e0b1d1d9b171",
   "metadata": {},
   "source": [
    "* [ ] design prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5bf3a43-d68d-4415-bb28-c321c42870b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt for question-answering\n",
    "# N.B. for non-US LangSmith endpoints, you may need to specify\n",
    "# api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from jinja2 import Environment, BaseLoader\n",
    "import textwrap\n",
    "\n",
    "jinja2_prompt = \"\"\"\\\n",
    "{% for part in parts %}\n",
    "Here is the No. {{ part.ordinal }} part of a tech credit\n",
    "Descirption:\n",
    "{{ part.tech_credit }}\n",
    "\n",
    "Example code for that tech credit:\n",
    "{{ part.context_code }}\n",
    "\n",
    "Here is the code from user:\n",
    "{{ part.user_code }}\n",
    "{% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "user_prompt_template = Environment(loader=BaseLoader).from_string(jinja2_prompt)\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an assistant for identifying technical credit. Use the following pieces \\\n",
    "                of retrieved context to answer the question. If you don't know the answer, just \\\n",
    "                say that you don't know. For each code snippet, use three sentences maximum and \\\n",
    "                keep the answer concise.\"),\n",
    "    (\"user\", textwrap.dedent(\"\"\"\\\n",
    "                Some documentation about tech credit:\n",
    "                {context_doc}\n",
    "\n",
    "                The following are snippets of codes that are most similar to example codes of \n",
    "                tech credits.\n",
    "                {rendered}\n",
    "                \n",
    "                Question: {question}\n",
    "                Answer:\n",
    "                \"\"\"))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105bf4cf-a257-4323-99b7-45d4fd92d73b",
   "metadata": {},
   "source": [
    "* [ ] collect metadata from code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba89a7a6-3c3f-4969-bdff-5cc6627d330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_unique_pairs(documents):\n",
    "    \"\"\"\n",
    "    Collect unique concatenated 'tech_credit: description' strings from document metadata.\n",
    "\n",
    "    Args:\n",
    "        documents (list[dict]): A list of Document objects, each with a 'metadata' field.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of unique 'tech_credit: description' strings.\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "\n",
    "    for doc in documents:\n",
    "        metadata = doc.metadata\n",
    "        credit = metadata.get(\"tech_credit\")\n",
    "        description = metadata.get(\"tech_credit_description\")\n",
    "        if credit and description:\n",
    "            combined = f\"{credit}: {description}\"\n",
    "            seen.add(combined)\n",
    "\n",
    "    return list(seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67ceec1c-ca5f-481d-a56c-50a0a00c9341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "from typing import List\n",
    "\n",
    "def load_repo(url: str, branch: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads all Python files from a GitHub repository using the repository URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The full URL of the GitHub repository (e.g., \"https://github.com/org/project\").\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of loaded document objects (the format depends on GithubFileLoader).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the URL is not a valid GitHub repo URL.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    if parsed.netloc != \"github.com\":\n",
    "        raise ValueError(f\"URL is not a github.com repo: {url}\")\n",
    "\n",
    "    # The path is like '/org/project' or '/org/project/'\n",
    "    path_parts = parsed.path.strip('/').split('/')\n",
    "    if len(path_parts) < 2:\n",
    "        raise ValueError(f\"Invalid GitHub repository URL: {url}\")\n",
    "    repo_name = '/'.join(path_parts[:2])  # Only org/project, ignore any deeper paths\n",
    "\n",
    "    loader = GithubFileLoader(\n",
    "        repo=repo_name,\n",
    "        branch=branch,\n",
    "        # access_token=ACCESS_TOKEN,\n",
    "        github_api_url=\"https://api.github.com\",\n",
    "        file_filter=lambda file_path: not file_path.startswith(\"tests/\") and file_path.endswith(\".py\"),\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def split_documents(documents: List[Document]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits Python code documents into code snippets and prepends the code structure as a comment header.\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): List of Document objects containing Python code.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: List of new strings, each with a code structure comment followed by the code snippet.\n",
    "    \"\"\"\n",
    "    python_splitter = TiktokenSplitter(Language.Python, max_size=200)\n",
    "\n",
    "    all_splits = []\n",
    "    for doc in documents:\n",
    "        splits = python_splitter.split(doc.page_content.encode(\"utf-8\"))\n",
    "        for snippet in splits:\n",
    "            # delete the header for now, only splitting the literal source code\n",
    "            # header = (\n",
    "            #    \"# ===== code structure =====\\n\" +\n",
    "            #    \"\\n\".join(f\"# {line}\" for line in snippet.subtree.splitlines()) +\n",
    "            #    \"\\n\\n\"\n",
    "            #)\n",
    "            all_splits.append(snippet.text)\n",
    "    return all_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e7fbf07-ec97-4bea-afe6-ba708826ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "from typing import Callable\n",
    "from statistics import mean, median # for later usage of different similarity score\n",
    "\n",
    "def default_min_score_fn(results: list[tuple[Document, float]]) -> float:\n",
    "    \"\"\"Default scoring function: returns the minimum score.\"\"\"\n",
    "    return min(score for _, score in results)\n",
    "\n",
    "def top_k_similar_queries(\n",
    "    queries: list[str],\n",
    "    vectorstore,\n",
    "    k: int = 3,\n",
    "    scoring_fn: Callable[[list[tuple[Document, float]]], float] = default_min_score_fn,\n",
    "    top_docs_per_query: int = 4\n",
    ") -> list[tuple[str, list[Document], float]]:\n",
    "    \"\"\"\n",
    "    Executes similarity_search_with_score for each query and returns top-k queries\n",
    "    sorted by a user-defined scoring function.\n",
    "\n",
    "    Args:\n",
    "        queries: List of query strings.\n",
    "        vectorstore: A LangChain-compatible vector store.\n",
    "        k: Number of top results to return.\n",
    "        scoring_fn: Function that maps a list of (Document, score) to a float score.\n",
    "        top_docs_per_query: Number of documents to retrieve for each query.\n",
    "\n",
    "    Returns:\n",
    "        A list of (query, documents, aggregated_score) sorted by aggregated_score descending.\n",
    "    \"\"\"\n",
    "    heap = []\n",
    "\n",
    "    for query in queries:\n",
    "        results = vectorstore.similarity_search_with_score(query, k=top_docs_per_query)\n",
    "        if not results:\n",
    "            continue\n",
    "\n",
    "        agg_score = scoring_fn(results)\n",
    "        \n",
    "        # if agg_score < 0.65:\n",
    "        #    continue\n",
    "    \n",
    "        docs = [doc for doc, _ in results]\n",
    "\n",
    "        # Use negative score to simulate a max-heap\n",
    "        heapq.heappush(heap, (-agg_score, query, docs))\n",
    "        if len(heap) > k:\n",
    "            heapq.heappop(heap)\n",
    "\n",
    "    # Return sorted results: highest score first\n",
    "    top_k = sorted([(-score, query, docs) for score, query, docs in heap], reverse=True)\n",
    "    return [(query, docs, score) for score, query, docs in top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad89e54e-0483-47d8-bda7-45e4ecec352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context_doc: List[Document]\n",
    "    # a part that contains example codes, user code, tech credit and index order\n",
    "    parts: list[dict]\n",
    "    url: str # user code\n",
    "    branch: str\n",
    "    answer: str\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    repo_splits = split_documents(load_repo(state[\"url\"], state[\"branch\"]))\n",
    "    retrieved_docs = top_k_similar_queries(repo_splits, vector_store)\n",
    "    parts = [\n",
    "        {\n",
    "            \"ordinal\": i + 1,\n",
    "            \"tech_credit\": \"\\n\".join(collect_unique_pairs(context_code)),\n",
    "            \"user_code\": user_code,\n",
    "            \"context_code\": \"\\n\\n\".join(doc.page_content for doc in context_code)\n",
    "        }\n",
    "        for i, (user_code, context_code, _) in enumerate(retrieved_docs)\n",
    "    ]\n",
    "    return {\"parts\": parts}\n",
    "\n",
    "def retrieve_doc(state: State):\n",
    "    retrieved_doc = document_store.similarity_search(state[\"question\"])\n",
    "    return {\"context_doc\": retrieved_doc}\n",
    "    \n",
    "def generate(state: State):\n",
    "    doc_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context_doc\"])\n",
    "    user_prompt = user_prompt_template.render(parts=state[\"parts\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"rendered\": user_prompt, \n",
    "                              \"context_doc\": doc_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_node(retrieve_doc)\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph_builder.add_edge(START, \"retrieve_doc\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c890ef-9f4b-4f62-929b-64bc5b33b4c4",
   "metadata": {},
   "source": [
    "# Ask Question Part\n",
    "## Circuit Breaker\n",
    "## MVC model\n",
    "## Iterator pattern\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb72313-9804-4db0-a3fe-ccd2841f9c6a",
   "metadata": {},
   "source": [
    "* [ ] Ask questions and test RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e17b8816-0600-47bb-8371-d010eaba2ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided documentation and code snippets, the repository appears to use the following technical credits:\n",
      "\n",
      "1. Circuit Breaker Pattern: This is the primary technical credit in the code. The circuit breaker enhances system resilience by dynamically detecting service failures and preventing cascading issues in distributed systems. It provides a mechanism to handle potential service failures gracefully, protecting the overall system from complete breakdown.\n",
      "\n",
      "2. Platform Abstraction Layer: Although not directly shown in the code, the documentation mentions creating abstraction layers around third-party products and libraries. This approach protects application code from external API changes and makes system modifications more manageable.\n",
      "\n",
      "3. Error Handling and Fallback Mechanisms: The code includes sophisticated error handling with multiple states (open, closed, half-open) and fallback strategies. For instance, the CircuitRedisStorage has a fallback circuit state if Redis connections fail.\n",
      "\n",
      "4. Flexible Import Handling: The code demonstrates robust import handling with try-except blocks for optional dependencies like Tornado and Redis, allowing the library to gracefully handle different environment configurations.\n",
      "\n",
      "These technical credits aim to improve system reliability, maintainability, and adaptability by implementing smart design patterns and error management strategies.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"Tell me what tech credits does the repo possibly use?\",\n",
    "\"url\": \"https://github.com/danielfm/pybreaker\", \"branch\": \"main\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb5e3468-c0b0-4ed2-9f12-9eaaa9403acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided code and documentation, this repository appears to use the Circuit Breaker pattern as a technical credit. Here's a detailed explanation:\n",
      "\n",
      "1. Circuit Breaker Pattern:\n",
      "- Purpose: Enhance system resilience by dynamically detecting service failures and preventing cascading issues, especially in distributed systems.\n",
      "- Key Implementation Details:\n",
      "   - Provides a mechanism to detect and handle potential service failures\n",
      "   - Implements different states (closed, open, half-open) for managing service calls\n",
      "   - Prevents repeated calls to failing services\n",
      "   - Includes timeout and fallback mechanisms\n",
      "\n",
      "2. Specific Technical Credit Characteristics:\n",
      "- Dynamic failure detection\n",
      "- Automatic service call protection\n",
      "- Graceful degradation through timeout and backup function mechanisms\n",
      "- State management for service resilience\n",
      "\n",
      "The code demonstrates multiple implementations of circuit breaker concepts:\n",
      "- State management classes (CircuitClosedState, CircuitOpenState)\n",
      "- Listener mechanisms for tracking call success/failure\n",
      "- Timeout and fallback function handling\n",
      "- Configurable failure thresholds and reset timeouts\n",
      "\n",
      "This implementation exemplifies technical credit by:\n",
      "- Reducing system vulnerability to cascading failures\n",
      "- Providing a reusable pattern for handling service unreliability\n",
      "- Enabling more robust and resilient distributed system design\n",
      "\n",
      "The implementation follows the description in the documentation about technical credit creating design decisions that ease future modifications and protect system reliability.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"Tell me what tech credits does the repo possibly use?\",\n",
    "\"url\": \"https://github.com/zacernst/circuit_breaker\", \"branch\": \"master\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a574c51-5eeb-4581-9f96-a5ad263b8941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided code snippets and documentation, the repository appears to use the following technical credits:\n",
      "\n",
      "1. Circuit Breaker Pattern\n",
      "The code demonstrates a robust implementation of the Circuit Breaker technical credit. This pattern enhances system resilience by:\n",
      "- Dynamically detecting service failures\n",
      "- Preventing cascading issues in distributed systems\n",
      "- Automatically managing system state (open, closed, half-open)\n",
      "- Protecting against overwhelming dependent services during failures\n",
      "\n",
      "The code includes multiple implementations that show:\n",
      "- Error handling mechanisms\n",
      "- Failure threshold tracking\n",
      "- State management\n",
      "- Async and sync call support\n",
      "- Custom error handling\n",
      "\n",
      "This technical credit exemplifies the documentation's description of creating design decisions that ease future modifications and protect the system from potential failures.\n",
      "\n",
      "2. Abstraction Layer\n",
      "While not as explicitly shown in the code, the documentation mentions platform abstraction layers as a form of technical credit. This would involve creating protective wrappers around third-party libraries to:\n",
      "- Isolate application code from external API changes\n",
      "- Simplify replacement of external dependencies\n",
      "- Reduce the impact of external library modifications\n",
      "\n",
      "The circuit breaker implementations demonstrate this principle by providing a consistent interface for handling service calls across different execution contexts.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"Tell me what tech credits does the repo possibly use?\",\n",
    "\"url\": \"https://github.com/fabfuel/circuitbreaker\", \"branch\": \"develop\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a5be63f-a708-4608-9d98-b913cca422d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided code snippets, this repository appears to be implementing a Circuit Breaker pattern, which is a classic technical credit strategy for enhancing system resilience in distributed systems. Here are the specific technical credit aspects:\n",
      "\n",
      "1. Circuit Breaker Pattern:\n",
      "- Dynamically detects service failures\n",
      "- Prevents cascading failures in distributed systems\n",
      "- Provides mechanisms to protect systems from potential service overloads\n",
      "- Implements state management (open, closed, half-open states)\n",
      "\n",
      "2. Error Handling and Resilience:\n",
      "- Tracks error and success counts\n",
      "- Implements configurable error thresholds\n",
      "- Allows custom error detection strategies\n",
      "- Supports exception allowlisting and denylisting\n",
      "- Provides mechanism to recover from failures\n",
      "\n",
      "3. Extensibility Features:\n",
      "- Supports listener mechanisms for tracking circuit breaker events\n",
      "- Allows custom error detection functions\n",
      "- Provides configurable recovery timeouts and strategies\n",
      "- Enables flexible initialization with various parameters\n",
      "\n",
      "The Circuit Breaker implementation demonstrates technical credit by:\n",
      "- Reducing system vulnerability to cascading failures\n",
      "- Providing a standardized approach to handling service unreliability\n",
      "- Creating an abstraction layer for error management\n",
      "- Facilitating system evolution and maintainability\n",
      "\n",
      "This approach aligns with the documentation's description of technical credit as a design decision that eases future modifications and protects the system's overall architecture.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"Tell me what tech credits does the repo possibly use?\",\n",
    "\"url\": \"https://github.com/etimberg/pycircuitbreaker\", \"branch\": \"master\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197d3c6f-9ed7-41a3-ac7a-f12633aa2330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = graph.invoke({\"question\": \"Tell me if the following is a tech credit and what do these 3 states do?\",\n",
    "# \"url\": \"https://github.com/pallets/jinja\", \"branch\": \"main\"})\n",
    "# print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe566a7-f5bf-4417-a90a-84ad342cf435",
   "metadata": {},
   "source": [
    "# Roadmap:\n",
    "\n",
    "1. Use a text embedding model.\n",
    "   + [X] gemini text-embedding-004\n",
    "2. Code splitter may not work properly.\n",
    "   + [X] change to code-splitter that works better.\n",
    "3. [X] Switch in memory vector storage to a vector database\n",
    "   + [X] code vector database with additional metadata for tech credit\n",
    "   + [X] document vector database with academic context about tech credit\n",
    "4. [ ] Load more standard example codes for tech credit (~20 more examples)\n",
    "5. [ ] Load more documents (academic contexts) for technical credit (3~5 related academic paper)\n",
    "6. [ ] Allow user to ask about a repository, instead of small snippets of codes\n",
    "   * [ ] load, chunk, split, embed and vectorize a repo\n",
    "   * [ ] similarity search and filter user code by similarity score compared with example code\n",
    "   * [ ] batch process to LLM\n",
    "   * [ ] organize response and answers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
